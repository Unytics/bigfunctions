type: function_py
category: AI
author:
  name: Paul Marcombes
  url: https://www.linkedin.com/in/paul-marcombes
  avatar_url: "https://lh3.googleusercontent.com/a-/ACB-R5RDf2yxcw1p_IYLCKmiUIScreatDdhG8B83om6Ohw=s260"
description: |
  Ask Anything!

  Google Generative AI `bison` model will get you an answer.
arguments:
  - name: question
    type: string
output:
  name: answer
  type: string
examples:
  - description: "Clean data"
    arguments:
      - |

        '''
        Question: what is the country from the following user input: 'I live in frace' ?
        Answer: formatted as alpha three code
        '''
    output: "FRA"
    region: ALL
  - description: "Generate SQL"
    arguments:
      - |

        '''
        Question: get the 10 articles which generated the most revenue in 2023
        Table: sales
        Columns: product_id, price, quantity, timestamp
        Answer: bigquery sql query
        '''
    output: |
      SELECT product_id, SUM(price * quantity) AS revenue
      FROM sales
      WHERE timestamp BETWEEN '2023-01-01' AND '2023-12-31'
      GROUP BY product_id
      ORDER BY revenue DESC
      LIMIT 10
    region: ALL
code: |
  import re
  import vertexai
  from vertexai.preview.language_models import TextGenerationModel

  model = 'text-bison@001'
  location = 'us-central1'

  question = question or ''
  question = re.sub(r'^[ \t]+', '', question)
  question = re.sub(r'[ \t]+$', '', question)
  question = re.sub(r'[ \t]+', ' ', question)
  question = question.strip()
  if not question:
      return

  cache_key = hash(question)
  cached = CACHE.get(cache_key)
  if cached:
      return cached

  if len(question) > 1000:
      raise QuotaException(f'Your question contains {len(question)} characters but it is limited to 1000 characters for now.')

  vertexai.init(location=location)
  model = TextGenerationModel.from_pretrained(model)
  response = model.predict(
      question,
      temperature=0.2,
      max_output_tokens=256,
      top_k=0.8,
      top_p=40,
  )
  answer = response.text
  if not answer:
      return 'GOOGLE LANGUAGE MODEL QUOTA REACHED'
  CACHE[cache_key] = answer
  return answer
requirements: |
  google-cloud-aiplatform
quotas:
  max_rows_per_user_per_day: 100
  max_rows_per_query: 1

