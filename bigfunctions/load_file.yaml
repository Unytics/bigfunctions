type: function_py
category: get_data
author:
  name: Antoine Giraud
  url: https://www.linkedin.com/in/antgiraud/
  avatar_url: "https://media.licdn.com/dms/image/C4D03AQG2Orctig4ycg/profile-displayphoto-shrink_800_800/0/1532385599460?e=1727308800&v=beta&t=jonuDcwGKpHZWsarmsIJ1AHm55WQsbMB1qQyU6p7alY"
description: |
  Download web file into `destination_table` (append)

  Under the hood : [DuckDB](https://duckdb.org/) fetches your files thanks to [ibis](https://ibis-project.org/).

  ![graph load file](./load_file.png)

  Available types:

  * csv : cf. ibis's [read_csv](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_csv)
  * json : cf. ibis's [read_json](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_json)
  * parquet : cf. ibis's [read_parquet](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_parquet)
  * delta : cf. ibis's [read_delta](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_delta)
  * geo : cf. ibis's [read_geo](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_geo) cf. DuckDB's [ST_READ](https://duckdb.org/docs/extensions/spatial/functions#st_read) in spatial extention : based on [GDAL](https://gdal.org/en/latest/index.html) translator
      * with this you can read .xls, .xlsx, .shp ...

  > **Requirements**
  >
  > You must create the `destination_dataset` and give `dataEditor` access to `bigfunction@bigfunctions.iam.gserviceaccount.com` before calling this function.
  > You can do this by executing:
  >
  > ```sql
  > -- Create Destination Dataset
  > create schema `your_project.your_dataset`;
  >
  > -- Grant Access to Destination Dataset
  > grant `roles/bigquery.dataEditor`
  > on schema `your_project.your_dataset`
  > to 'serviceAccount:bigfunction@bigfunctions.iam.gserviceaccount.com';
  > ```
arguments:
  - name: url
    type: string
  - name: file_type
    type: string
  - name: destination_table
    type: string
  - name: options
    type: string
output:
  name: status
  type: string
examples:
  - description: "load random csv"
    arguments:
      - "'https://raw.githubusercontent.com/AntoineGiraud/dbt_hypermarche/refs/heads/main/input/achats.csv'"
      - "'csv'"
      - "'your_project.your_dataset.random_sales'"
      - "null"
    output: "ok"
  - description: "load json - french departements"
    arguments:
      - "'https://geo.api.gouv.fr/departements?fields=nom,code,codeRegion,region'"
      - "'json'"
      - "'your_project.your_dataset.dim_french_departements'"
      - "null"
    output: "ok"
  - description: "load parquet on Google Cloud Storage"
    arguments:
      - "'gs://bike-sharing-history/toulouse/jcdecaux/2024/Feb.parquet'"
      - "'parquet'"
      - "'your_project.your_dataset.station_status'"
      - "null"
    output: "ok"
  - description: "load xls or xlsx"
    arguments:
      - "'https://github.com/AntoineGiraud/dbt_hypermarche/raw/refs/heads/main/input/Hypermarche.xlsx'"
      - "'xlsx"
      - "'your_project.your_dataset.hypermarche_retours'"
      - '{"layer":"Retours", "open_options" = ["HEADERS=FORCE"]}'
    output: "ok"
  - description: "load french tricky csv"
    arguments:
      - "'https://www.data.gouv.fr/fr/datasets/r/eae12477-d02e-441b-91f7-a2eeae98c5c7'"
      - "'csv'"
      - "'your_project.your_dataset.dim_french_postalcodes'"
      - |
        '''{
          "columns": {
              "code_commune_insee": "VARCHAR",
              "nom_commune_insee": "VARCHAR",
              "code_postal": "VARCHAR",
              "lb_acheminement": "VARCHAR",
              "ligne_5": "VARCHAR"
          },
          "delim": ";",
          "skip": 1
        }'''
code: |
  if not destination_table:
    return "invalid destination_table"
  if file_type not in ("csv", "json", "parquet", "geo", "delta", "xls", "xlsx"):
    return "invalid type: accepted values: csv, json, parquet, geo, delta, xls, xlsx"
  if not url:
    return "invalid url: it is null or empty"
  if not url.startswith( ("http://", "https://", "hf://", "s3://", "az://", "abfss://", "gs://") ):
    return "invalid url: wrong start. accepted values: 'http://', 'https://', 'hf://', 's3://', 'az://', 'abfss://', 'gs://'"

  import json
  import ibis
  import google.cloud.bigquery
  import google.api_core.exceptions
  from slugify import slugify

  if file_type in ('xls', 'xlsx'):
    file_type = 'geo' # use same DuckDB's `st_read` function

  if file_type in ('geo'):
    import urllib.request

  # try to parse options as json
  try:
    options = json.loads(options) if options else {}
  except Exception as e:
    return "invalid json options " + repr(e)

  # DuckDB st_read needs local file
  if file_type in ('geo'):
    urllib.request.urlretrieve(url, './geo_file')
    url = './geo_file'

  # let's fetch the file with ibis & DuckDB
  try:
    con = ibis.duckdb.connect()
    read = {
      "csv": con.read_csv,
      "json": con.read_json,
      "parquet": con.read_parquet,
      "geo": con.read_geo,
      "delta": con.read_delta
    }[file_type]
    df = read(url, **options).to_pandas()
  except Exception as e:
    return "duckdb fetch failure" + repr(e)

  # let's load the dataframe to BigQuery
  bigquery = google.cloud.bigquery.Client()
  try:
    df.columns = [slugify(col, separator="_") for col in df.columns]
    bigquery.load_table_from_dataframe(df, destination_table).result()
  except (google.api_core.exceptions.Forbidden, google.api_core.exceptions.NotFound, google.api_core.exceptions.PermissionDenied) as e:
    assert False, f'Service Account `{get_current_service_account()}` does not have data-editor permission for given destination dataset (or the dataset does not exsit). Please add it'
  return 'ok'
requirements: |
  ibis-framework[duckdb]
  google-cloud-bigquery
  geopandas
  python-slugify
quotas:
  max_rows_per_query: 10
cloud_run:
  memory: 2048Mi
  concurrency: 1
  max_instances: 10
