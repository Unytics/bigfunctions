type: procedure
category: get_data
author:
  name: Antoine Giraud
  url: https://www.linkedin.com/in/antgiraud/
  avatar_url: "https://media.licdn.com/dms/image/v2/C4D03AQG2Orctig4ycg/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1532385599321?e=1733961600&v=beta&t=HTHvVZUKUhoH-U07O1tXPPuqN37EgCtGhxM_PuYFQjQ"
description: |
  Download web file into `destination_table`

  ![graph load file](./load_file.png)

  This function extends `load_file_into_my_unytics_data` function and will:

  1. Create a dataset for you in `my-unytics-data` project that will hold a temporary copy of the table.
      - *The dataset will be named after your email and suffixed with `_load_file` + the dataset location (example `eu`).*
      - *For instance, if your email is `firstname.lastname@domain.com`, the created dataset will be `my-unytics-data.firstname_DOT_lastname_AT_domain_DOT_com__load_file__eu`*
  2. Set the default expiration duration of 1h in the dataset so that any table will be deleted after 1h.
  3. Grant you table read permission and table delete permission in this dataset.
      - *Only you and this function have access to it.*
  4. Download the file using [ibis](https://ibis-project.org/) with [DuckDB](https://duckdb.org/).
      - *`options` argument is passed to the duckdb read function.*
  5. Load the data to a new table named after the url in the created dataset.
      - *The table will be automatically deleted after one hour. You can also delete it manually.*
  6. Copy the table to `destination_table`
  7. Delete the temporary table from the dataset.

  Available types:

  - **csv** : *[doc](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_csv)*
  - **json** : *[doc](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_json)*
  - **parquet** : *[doc](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_parquet)*
  - **delta** : *[doc](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_delta)*
  - **geo** : *[doc](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_geo)*. (this uses GDAL under the hood and enable you to also read **.xls**, **.xlsx**, **.shp** ...)
arguments:
  - name: url
    type: string
  - name: file_type
    type: string
  - name: destination_table
    type: string
  - name: options
    type: string
output:
  name: status
  type: string
examples:
  - description: "load random csv"
    arguments:
      - "\n  'https://raw.githubusercontent.com/AntoineGiraud/dbt_hypermarche/refs/heads/main/input/achats.csv'"
      - "\n  'csv'"
      - "'your_project.your_dataset.random_sales'"
      - "null\n"
    output: |
      +--------+
      + status +
      +--------+
      +   ok   +
      +--------+
  - description: "load json - french departements"
    arguments:
      - "\n  'https://geo.api.gouv.fr/departements?fields=nom,code,codeRegion,region'"
      - "\n  'json'"
      - "'your_project.your_dataset.dim_french_departements'"
      - "null\n"
    output: |
      +--------+
      + status +
      +--------+
      +   ok   +
      +--------+
  - description: "load parquet on Google Cloud Storage"
    arguments:
      - "\n  'gs://bike-sharing-history/toulouse/jcdecaux/2024/Feb.parquet'"
      - "\n  'parquet'"
      - "'your_project.your_dataset.station_status'"
      - "null\n"
    output: |
      +--------+
      + status +
      +--------+
      +   ok   +
      +--------+
  - description: "load xls or xlsx"
    arguments:
      - "\n  'https://github.com/AntoineGiraud/dbt_hypermarche/raw/refs/heads/main/input/Hypermarche.xlsx'"
      - "\n  'xlsx'"
      - "'your_project.your_dataset.hypermarche_retours'"
      - "'{\"layer\":\"Retours\", \"open_options\": [\"HEADERS=FORCE\"]}'\n"
    output: |
      +--------+
      + status +
      +--------+
      +   ok   +
      +--------+
  - description: "load french tricky csv"
    arguments:
      - "\n  'https://www.data.gouv.fr/fr/datasets/r/323af5b8-7831-445b-9a46-d4da140b61b6'"
      - "\n  'csv'"
      - "\n  'your_project.your_dataset.dim_french_postalcodes'\n"
      - |
        \n'''{
          "columns": {
              "code_commune_insee": "VARCHAR",
              "nom_commune_insee": "VARCHAR",
              "code_postal": "VARCHAR",
              "lb_acheminement": "VARCHAR",
              "ligne_5": "VARCHAR"
          },
          "delim": ";",
          "skip": 1
        }'''\n
    output: |
      +--------+
      + status +
      +--------+
      +   ok   +
      +--------+
code: |
  declare table_id string;

  assert file_type in ('csv', 'json', 'parquet', 'geo', 'delta') as '`file_type` must be in (csv, json, parquet, geo, delta)';
  assert array_length(split(destination_table, '.')) = 3 as '`destination_table` must be like PROJECT.DATASET.TABLE';

  set table_id = (
    select {BIGFUNCTIONS_DATASET}.load_file_into_my_unytics_data(url, file_type, options)
  );

  execute immediate format(
    'create or replace table %s copy %s',
    destination_table, table_id
  );

  execute immediate format(
    'drop table %s',
    table_id
  );

  create or replace temp table bigfunction_result as
  select 'ok' as status;
